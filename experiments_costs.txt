TEST 20/09/2024

Executed eval_time_goal_gen.py on GPT-4o-mini
    - testing 34 scenarios x 5 iterations
    - 323 queries
    - 206.152 input tokens
    - 6.688 output tokens
    - COST: 0.03$

Executed eval_goal_gen.py on GPT-4o-mini
    - testing 35 scenarios
    - 72 queries
    - 45.861 input tokens
    - 1.775 output tokens
    - COST: 0.01$

TEST 25/09/2024

Executed eval_agent_adapt.py on GPT-4o-mini
    - Failed because of non deterministic answers
    - 17 queries
    - 7.376 input tokens
    - 1.026 output tokens
    - COST: <0.01$

TEST 28/09/2024
Executed eval_agent_adapt.py on GPT-4o-mini
    - 20 scenarios -> others failed because of non deterministic answers, but more answers where retrieved due to changes on code and adding some prompt
    - 21 queries (1 from first failure)
    - 9.875 input tokens
    - 631 output tokens
    - COST: <0.01$

Executed eval_agent_adapt.py on GPT-4o-mini
    - 93 scenarios -> the ones in the csv scenarios except the first 20
    - 93 queries
    - 43.866 input tokens
    - 2.950 output tokens
    - COST: <0.01$

TEST 11/10/2024
Executing tests in the new domain
    - Testing with new domain
    - X queries
    - X input tokens
    - X output tokens
    - COST: <0.01$

TEST 21/10/2024
Executing tests in Spanish
    - 34 scenarios
    - 28 queries
    - 17.712 input tokens
    - 1.164 output tokens
    - COST: <0.01$

Executing tests in Spanish
    - Failed 

TEST 23/10/2024
Other failed tests
    - 83 requests
    - 52.592 input tokens
    - 3.822 output tokens
    - COST: <0.01$

TEST 28/10/2024
Executing test for get plan
    - Due to error in model; we spent 0.15$ because it was using GPT-4
    - 32 queries to GPT-4

Executing test for get plan
    - IDK how many queries to GPT-4o-mini
    - <0.01$
