{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval time goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "file_2024 = 'outputs/eval_time_goal_gen20240920-142509.csv'\n",
    "file_2023 = 'results/eval_time_goal_gen20230720-142101.csv'\n",
    "\n",
    "# Read the CSV files into dataframes\n",
    "df_2024 = pd.read_csv(file_2024)\n",
    "df_2023 = pd.read_csv(file_2023)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean Time 2023 (s)': 3.243075455725193,\n",
       " 'Mean Time 2024 (s)': 3.9854286834597588,\n",
       " 'Mean Time Difference (s)': 0.7423532277345657,\n",
       " 'Median Time 2023 (s)': 1.528114557266235,\n",
       " 'Median Time 2024 (s)': 3.9244431257247925}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean dataframes by removing extra unnamed columns\n",
    "df_2023_clean = df_2023[['Goal Type', 'Goal number', 'INPUT - Goal', 'n_run', 'time (s)']]\n",
    "df_2024_clean = df_2024[['Goal Type', 'Goal number', 'INPUT - Goal', 'n_run', 'time (s)']]\n",
    "\n",
    "# Merge both dataframes on common columns: 'Goal Type', 'Goal number', 'INPUT - Goal', and 'n_run'\n",
    "comparison_df = pd.merge(df_2024_clean, df_2023_clean, on=['Goal Type', 'Goal number', 'INPUT - Goal', 'n_run'], suffixes=('_2024', '_2023'))\n",
    "\n",
    "# Calculate the difference in time (s)\n",
    "comparison_df['time_difference'] = comparison_df['time (s)_2024'] - comparison_df['time (s)_2023']\n",
    "\n",
    "# Show the first few rows of the comparison to verify\n",
    "# Perform general comparison\n",
    "# Calculate overall statistics for both years\n",
    "mean_time_2023 = comparison_df['time (s)_2023'].mean()\n",
    "mean_time_2024 = comparison_df['time (s)_2024'].mean()\n",
    "mean_time_difference = comparison_df['time_difference'].mean()\n",
    "\n",
    "# Calculate median times for both years\n",
    "median_time_2023 = comparison_df['time (s)_2023'].median()\n",
    "median_time_2024 = comparison_df['time (s)_2024'].median()\n",
    "\n",
    "# Calculate overall statistics for comparison\n",
    "comparison_stats = {\n",
    "    'Mean Time 2023 (s)': float(mean_time_2023),\n",
    "    'Mean Time 2024 (s)': float(mean_time_2024),\n",
    "    'Mean Time Difference (s)': float(mean_time_difference),\n",
    "    'Median Time 2023 (s)': float(median_time_2023),\n",
    "    'Median Time 2024 (s)': float(median_time_2024)\n",
    "}\n",
    "\n",
    "comparison_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Goal Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_new_5shot = \"results/complete_eval_goal_gen_5shot.csv\"\n",
    "df_new_2024 = \"outputs/eval_goal_gen20240920-160735.csv\"\n",
    "\n",
    "# Attempt to load the second file using a different encoding to avoid UnicodeDecodeError\n",
    "df_new_5shot = pd.read_csv(file_new_5shot, encoding='ISO-8859-1')\n",
    "df_new_2024 = pd.read_csv(df_new_2024, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>2024 (Mean)</th>\n",
       "      <th>5-shot (Mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Completeness</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.901150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Correctness</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.857242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test Success</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Metric  2024 (Mean)  5-shot (Mean)\n",
       "0  Completeness     0.529412       0.901150\n",
       "1   Correctness     0.529412       0.857242\n",
       "2  Test Success     1.000000       0.892915"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up relevant columns for both files\n",
    "# Focusing on unfiltered and ungrounded metrics for completeness, correctness, and test success in both files\n",
    "\n",
    "# 2024 data file columns to focus on\n",
    "df_new_2024_clean = df_new_2024[['METRIC 1: subgoal completeness - unfiltered and ungrounded', \n",
    "                                 'METRIC 2: subgoal correctness - unfiltered and ungrounded', \n",
    "                                 'METRIC 3: test success - unfiltered and ungrounded']].copy()\n",
    "\n",
    "df_new_2024_clean.columns = ['completeness', 'correctness', 'test_success']  # Renaming for simplicity\n",
    "\n",
    "# 5-shot data file columns to focus on\n",
    "df_new_5shot_clean = df_new_5shot[['METRIC 1: subgoal completeness - unfiltered and ungrounded', \n",
    "                                   'METRIC 2: subgoal correctness - unfiltered and ungrounded', \n",
    "                                   'METRIC 3: test success - unfiltered and ungrounded']].copy()\n",
    "\n",
    "df_new_5shot_clean.columns = ['completeness', 'correctness', 'test_success']  # Renaming for simplicity\n",
    "\n",
    "# Convert any 'manual_check' or invalid entries to NaN and convert to numeric\n",
    "df_new_2024_clean.replace('manual_check', pd.NA, inplace=True)\n",
    "df_new_5shot_clean.replace('manual_check', pd.NA, inplace=True)\n",
    "\n",
    "df_new_2024_clean = df_new_2024_clean.apply(pd.to_numeric, errors='coerce')\n",
    "df_new_5shot_clean = df_new_5shot_clean.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate overall statistics (mean) for both datasets\n",
    "comparison_summary = pd.DataFrame({\n",
    "    'Metric': ['Completeness', 'Correctness', 'Test Success'],\n",
    "    '2024 (Mean)': [df_new_2024_clean['completeness'].mean(), \n",
    "                    df_new_2024_clean['correctness'].mean(), \n",
    "                    df_new_2024_clean['test_success'].mean()],\n",
    "    '5-shot (Mean)': [df_new_5shot_clean['completeness'].mean(), \n",
    "                      df_new_5shot_clean['correctness'].mean(), \n",
    "                      df_new_5shot_clean['test_success'].mean()]\n",
    "})\n",
    "\n",
    "comparison_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric  2024 (Mean)  5-shot (Mean)\n",
      "0  Completeness     0.470588       0.843540\n",
      "1   Correctness     0.470588       0.937316\n",
      "2  Test Success     0.470588       0.926952\n"
     ]
    }
   ],
   "source": [
    "# Limpiar las columnas de interés\n",
    "df_new_2024_clean = df_new_2024[['METRIC 1: subgoal completeness', \n",
    "                                 'METRIC 2: subgoal correctness', \n",
    "                                 'METRIC 3: test success']].copy()\n",
    "\n",
    "df_new_5shot_clean = df_new_5shot[['METRIC 1: subgoal completeness', \n",
    "                                   'METRIC 2: subgoal correctness', \n",
    "                                   'METRIC 3: test success']].copy()\n",
    "\n",
    "# Renombrar columnas para mayor simplicidad\n",
    "df_new_2024_clean.columns = ['completeness', 'correctness', 'test_success']\n",
    "df_new_5shot_clean.columns = ['completeness', 'correctness', 'test_success']\n",
    "\n",
    "# Reemplazar valores no válidos como 'manual_check' por NaN y convertir a numérico\n",
    "df_new_2024_clean.replace('manual_check', pd.NA, inplace=True)\n",
    "df_new_5shot_clean.replace('manual_check', pd.NA, inplace=True)\n",
    "\n",
    "df_new_2024_clean = df_new_2024_clean.apply(pd.to_numeric, errors='coerce')\n",
    "df_new_5shot_clean = df_new_5shot_clean.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calcular estadísticas (medias) para ambas evaluaciones\n",
    "comparison_summary = pd.DataFrame({\n",
    "    'Metric': ['Completeness', 'Correctness', 'Test Success'],\n",
    "    '2024 (Mean)': [df_new_2024_clean['completeness'].mean(), \n",
    "                    df_new_2024_clean['correctness'].mean(), \n",
    "                    df_new_2024_clean['test_success'].mean()],\n",
    "    '5-shot (Mean)': [df_new_5shot_clean['completeness'].mean(), \n",
    "                      df_new_5shot_clean['correctness'].mean(), \n",
    "                      df_new_5shot_clean['test_success'].mean()]\n",
    "})\n",
    "\n",
    "# Mostrar el resumen\n",
    "print(comparison_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Agent Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.read_csv('outputs/eval_agent_adapt20240928-203445_complete.csv', encoding='ISO-8859-1')\n",
    "old = pd.read_csv('results/complete_eval_agent_adapt.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235294117647058\n",
      "0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "print(new[~(new['GT subgoals to disfavour'].isna())]['subgoals disfavour correctness'].mean())\n",
    "print(new[~(new['GT subgoals to disfavour'].isna())]['subgoals disfavour completeness'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n",
      "0.698125\n"
     ]
    }
   ],
   "source": [
    "print(old[~(old['GT subgoals to disfavour'].isna())]['subgoals disfavour correctness'].mean())\n",
    "print(old[~(old['GT subgoals to disfavour'].isna())]['subgoals disfavour completeness'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
